{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(binary_image, figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, utils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel CNN\n",
    "\n",
    "Alternative to Pixel RNN from [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf). \n",
    "\n",
    "On-line resources:\n",
    " * See for an existing PyTorch implementation https://github.com/jzbontar/pixelcnn-pytorch/blob/master/main.py\n",
    " * http://sergeiturukin.com/2017/02/22/pixelcnn.html for a nice walk-through\n",
    " * http://tinyclouds.org/residency/\n",
    " * https://tensorflow.blog/2016/11/29/pixelcnn-1601-06759-summary/ (in korean ;) ) \n",
    "\n",
    "The core ideas are the following:\n",
    "\n",
    "### Joint distribution of an image $\\mathbf{x}$ modelled as an autoregressive process\n",
    "\n",
    "Same model for PixelRNN and PixelCNN:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\prod_{i=1}^{n^2} p(x_i|x_{1}, \\dots, x_{i-1})$$\n",
    " \n",
    "![](http://sergeiturukin.com/assets/2017-02-22-183010_479x494_scrot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAElCAYAAACiZ/R3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABDFJREFUeJzt17Ftw0AUBUGeoRLk2CzC/VcgFqHcPZxTOxJpgNBCnolf\n8KPF3ZhzLgAVb88+AOAnUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlIuR8bX63Wu63rSKcAr\n27bta875/mh3KErrui632+3vVwH/1hjjvmfn+wakiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSI\nEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpA\niigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmi\nBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQ\nIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIoo\nASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSk\niBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpByefYBnGuM8ewT4BAvJSBFlIAUUQJSRAlIESUg\nRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRR\nAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlI\nESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgZcw594/H2D8G+G2bc34+\nGnkpASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEp\nogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgS\nkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCK\nKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIE\npIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAi\nSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigB\nKaIEpIgSkCJKQMrl4P5rWZb7GYcAL+9jz2jMOc8+BGA33zcgRZSAFFECUkQJSBElIEWUgBRRAlJE\nCUgRJSDlG/BoHd7iaQCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f0ff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def causal_mask(width, height, include_center=False):\n",
    "    mask = np.ones(shape=(width, height))\n",
    "    mask[height // 2, width // 2 + include_center:] = 0\n",
    "    mask[height // 2 + 1:] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "print(causal_mask(5, 5))\n",
    "show_as_image(causal_mask(5, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  1.,   2.,   3.],\n",
       "         [  4.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[ 10.,  11.,  12.],\n",
       "         [ 13.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[ 19.,  20.,  21.],\n",
       "         [ 22.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[ 28.,  29.,  30.],\n",
       "         [ 31.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels, in_channels, width, height = 2, 2, 3, 3\n",
    "\n",
    "conv_weights = 1 + np.arange(out_channels * in_channels * width * height).reshape((out_channels, in_channels, width, height))\n",
    "\n",
    "masked_weights = conv_weights * causal_mask(width, height)\n",
    "\n",
    "masked_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully convolutional network preserving spatial resolution\n",
    "\n",
    "Input to output map      |  Output distribution\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://tensorflowkorea.files.wordpress.com/2016/11/pixel-cnn1.png)  |  ![](http://tinyclouds.org/residency/pixelcnn.png)\n",
    "\n",
    "Quite a counter-intuitive model:\n",
    "\n",
    " * Convolutional layers bottom to top!\n",
    " * Last layer with `kernel_size=1` and outputs $ n_W \\times n_H \\times n_{pixels}$ logits, inferring $p(\\mathbf{x})$ in one forward pass (during training)\n",
    " * Representation of dimension `n_channels` output by each layer anologous to RNN's internal state vector $\\mathbf{h}$\n",
    " * Necessary to stack enough layers (and/or dillatations) to augment the \"receptive field\" so that output pixels can be influenced by the whole image\n",
    " \n",
    "\n",
    "Below is a minimalistic implementation for 0/1 pixels without many of the bells and whistles of the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on a simple generative model of LCD digits\n",
    "\n",
    "From https://gist.github.com/benjaminwilson/b25a321f292f98d74269b83d4ed2b9a8#file-lcd-digits-dataset-nmf-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA2pJREFUeJzt3LGRAjEQAMFbijw+/ww+HXxy0IcAV3DAM922jDWmVDIk\nzVprg4rTuweAVxI8KYInRfCkCJ4UwZMieFIET4rgSdkV/Mz8HjUIPOLeNmfP1YKZcQ+Bj7XWmltr\nHGlIETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgifl/O4BjuAL\n8OeauflU9N+ww5MieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIE\nT4rgSfnKR9zf9OiY57LDkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4\nUgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQI\nnhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwp\ngidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRP\niuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwp553rr9u2XY4YBB70\nc8+iWWsdPQh8DEcaUgRPiuBJETwpgidF8KQInhTBkyJ4Uv4AmrcVqoHJrzkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11750f198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CELL_LENGTH = 4\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 2 * CELL_LENGTH + 5, CELL_LENGTH + 4\n",
    "\n",
    "def vertical_stroke(rightness, downness):\n",
    "    \"\"\"\n",
    "    Return a 2d numpy array representing an image with a single vertical stroke in it.\n",
    "    `rightness` and `downness` are values from [0, 1] and define the position of the vertical stroke.\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 2\n",
    "    j = rightness * (CELL_LENGTH + 1) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i + np.arange(CELL_LENGTH), j] = 1.\n",
    "    return x\n",
    "\n",
    "def horizontal_stroke(downness):\n",
    "    \"\"\"\n",
    "    Analogue to vertical_stroke, but it returns horizontal strokes.\n",
    "    `downness` is here a value in [0, 1, 2].\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i, 2 + np.arange(CELL_LENGTH)] = 1.\n",
    "    return x\n",
    "\n",
    "show_as_image(horizontal_stroke(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA4xJREFUeJzt3cFtg0AQQFEmch/pv4O0k3t62JRgW4ZA/N87I2skvlYc\nGDxrrQ0qPs4eAP6S4EkRPCmCJ0XwpAieFMGTInhSBE/KU8HPzNdRg8ArHm1znnm1YGa8h8BlrbXm\n3jUeaUgRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ+V29gBH\n8Anwfc3cXRX9N5zwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAie\nFMGTInhS3nKJ+52Wjq/giKX4s+6RE54UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8\nKYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIE\nT4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4U\nwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYIn\nRfCkCJ4UwZNyO3sArm9mzh5hN054UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQI\nnhTBkyJ4UgRPiuBJETwplri5a621+2+etRjuhCdF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4\nUgRPiuBJETwpgidF8KQInpTb2QNwfTNz9gi7ccKTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K\n4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGT\nInhSBE+K4EkRPCmCJ0XwpAieFMGT8uw/cf9s2/Z9xCDwos9HLpq11tGDwGV4pCFF8KQInhTBkyJ4\nUgRPiuBJETwpgiflFz6hG7aPxRw7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f077f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_STROKES = np.asarray(\n",
    "    [horizontal_stroke(k) for k in range(3)] + [vertical_stroke(k, l) for k in range(2) for l in range(2)])\n",
    "\n",
    "DIGITS_STROKES = np.array([[0, 2, 3, 4, 5, 6], [5, 6], [0, 1, 2, 4, 5], [0, 1, 2, 5, 6], [1, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 3, 4, 6], [0, 5, 6], np.arange(7), [0, 1, 2, 3, 5, 6]])\n",
    "\n",
    "def random_digits(strokes=BASE_STROKES, digit_as_strokes=DIGITS_STROKES, fixed_label=None):\n",
    "    label = fixed_label if fixed_label is not None else np.random.choice(len(digit_as_strokes))\n",
    "    combined_strokes = strokes[digit_as_strokes[label], :, :].sum(axis=0)\n",
    "    return combined_strokes, label\n",
    "\n",
    "\n",
    "x, label = random_digits()\n",
    "print(label)\n",
    "show_as_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " (0 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " \n",
       " (1 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " \n",
       " (2 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 3x1x13x8], \n",
       "  4\n",
       "  9\n",
       "  8\n",
       " [torch.LongTensor of size 3x1]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LcdDigits(Dataset):\n",
    "\n",
    "    def __init__(self, n_examples):\n",
    "        digits, labels = zip(*[random_digits() for _ in range(n_examples)])\n",
    "        self.digits = np.asarray(digits, dtype=np.float64)\n",
    "        self.labels = np.asarray(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        digit_with_channel = self.digits[idx][np.newaxis, :, :]\n",
    "        \n",
    "        return torch.from_numpy(digit_with_channel).float(), torch.from_numpy(np.array([self.labels[idx]]))\n",
    "\n",
    "next(b for b in DataLoader(LcdDigits(128), batch_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iter [1/50] Loss: 0.7986\n",
      "Epoch [2/15], Iter [1/50] Loss: 0.1839\n",
      "Epoch [3/15], Iter [1/50] Loss: 0.0476\n",
      "Epoch [4/15], Iter [1/50] Loss: 0.0349\n",
      "Epoch [5/15], Iter [1/50] Loss: 0.0300\n",
      "Epoch [6/15], Iter [1/50] Loss: 0.0276\n",
      "Epoch [7/15], Iter [1/50] Loss: 0.0265\n",
      "Epoch [8/15], Iter [1/50] Loss: 0.0263\n",
      "Epoch [9/15], Iter [1/50] Loss: 0.0255\n",
      "Epoch [10/15], Iter [1/50] Loss: 0.0255\n",
      "Epoch [11/15], Iter [1/50] Loss: 0.0249\n",
      "Epoch [12/15], Iter [1/50] Loss: 0.0248\n",
      "Epoch [13/15], Iter [1/50] Loss: 0.0249\n",
      "Epoch [14/15], Iter [1/50] Loss: 0.0247\n",
      "Epoch [15/15], Iter [1/50] Loss: 0.0249\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "train_dataset = LcdDigits(BATCH_SIZE * 50)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, N_EPOCHS, i+1, len(train_dataset)//BATCH_SIZE, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentially generating new samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA55JREFUeJzt3cFtAjEQQFEmoo/030HayT09OCUAYjcm/PfOK8uSv6w9\n7MCstS5Q8bF7A/CXBE+K4EkRPCmCJ0XwpAieFMGTInhSHgp+Zr7O2gg8494255FPC2bGdwi8rLXW\n3HrGKw0pgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KRcd2/g\nDH4C/FgzN0dF/w03PCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmC\nJ0XwpAielO1D3GcMXL/T0DHHcsOTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0Xw\npAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkR\nPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhS\nBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAie\nFMGTInhSrrs3MDOHr7nWOnzNsjPOaBc3PCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGT\nInhSBE+K4EkRPCmCJ0XwpAielO1D3GcMXL/T0PEreKczcsOTInhSBE+K4EkRPCmCJ0XwpAieFMGT\nInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0Xw\npAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkR\nPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhS\nBE+K4EkRPCmCJ0XwpAieFMGTInhSBE/KdfcGZubwNddah69ZdsYZ7eKGJ0XwpAieFMGTInhSBE+K\n4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTsn2I+wzvNHTMsdzwpAieFMGTInhS\nBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSHh3i/rlcLt9nbASe9HnP\nQ+Nv2inxSkOK4EkRPCmCJ0XwpAieFMGTInhSBE/KL7ETJLjeeeqBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b9e6a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(n_samples, starting_point=(0, 0), starting_image=None):\n",
    "\n",
    "    samples = torch.from_numpy(\n",
    "        starting_image if starting_image is not None else np.zeros((n_samples * n_samples, 1, IMAGE_WIDTH, IMAGE_HEIGHT))).float()\n",
    "\n",
    "    cnn.train(False)\n",
    "\n",
    "    for i in range(IMAGE_WIDTH):\n",
    "        for j in range(IMAGE_HEIGHT):\n",
    "            if i < starting_point[0] or (i == starting_point[0] and j < starting_point[1]):\n",
    "                continue\n",
    "            out = cnn(Variable(samples, volatile=True))\n",
    "            probs = F.softmax(out[:, :, i, j]).data\n",
    "            samples[:, :, i, j] = torch.multinomial(probs, 1).float()\n",
    "    return samples.numpy()\n",
    "\n",
    "show_as_image(np.squeeze(generate_samples(n_samples=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or completing existing cropped image\n",
    "\n",
    " * $0, 8, 9$ and $2, 3, 7$ undistinguishable early one\n",
    " * Very small amount of noise (jitter) in samples\n",
    " * The last horizontal bar is hard to predict as it depends on the 1st horizontal bar\n",
    " * ($4, 9$) sometimes lead to incomplete or erroneous images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABwCAYAAAAdSHSxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAz5JREFUeJzt3cFV4zAUhlFpDi3MepqgLacZ10UR7OlBs2WVOImj2O+/\ndy1wXmQ43zFHpI8xGgBAqj/vfgEAAO8khgCAaGIIAIgmhgCAaGIIAIgmhgCAaGIIAIgmhgCAaGII\nAIgmhgCAaB/3LO69++wOAOAsfsYYf28t8mQIAKjqe8siMQQARBNDAEA0MQQARBNDAEC0u06TPWqM\nuYfQeu9Tr1fdzP2bvXfuzXOrvn/m20/l2Voz37M8GQIAookhACCaGAIAookhACCaGAIAookhACDa\nbkfrrx2zO9KRvwpHm2fPN/M9qzxba6+Z70j3e/X5XqH6fNec/XfLkfbOfM/xZAgAiCaGAIBoYggA\niCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiLbbp9YfyeVyefdL\neKnK81WerTXzwbtUvzfN9xxPhgCAaGIIAIgmhgCAaGIIAIgmhgCAaGIIAIi229H66sf64Kiq/+yZ\nj6OqvnfV5/vNkyEAIJoYAgCiiSEAIJoYAgCiiSEAIJoYAgCi9THG9sW9b19MGcuyTLvWuq7TrtXa\n3NlaM9/ezLev2fNVVn3vTjTf1xjj89YiT4YAgGhiCACIJoYAgGhiCACIJoYAgGhiCACIttvR+mvH\n7I505K/C0dFXzFf9PTuK6u+z+Y7xPR91pNeyN3t3bk/M52g9AMAtYggAiCaGAIBoYggAiCaGAIBo\nYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggA\niCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBofYyxfXHv2xf/sizLI1/2\nsHVdp16vupn7N3vv3JvnVn3/qs9XWfW9O9F8X2OMz1uLPBkCAKKJIQAgmhgCAKKJIQAgmhgCAKKJ\nIQAg2pSj9QAAb+BoPQDALWIIAIgmhgCAaGIIAIgmhgCAaGIIAIj2cef6n9ba9yteCADAzv5tWXTX\n/xkCAKjGn8kAgGhiCACIJoYAgGhiCACIJoYAgGhiCACIJoYAgGhiCACIJoYAgGj/AY5p07feFH+j\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179d58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "starting_point = (4, 3)\n",
    "starting_image = random_digits(fixed_label=0)[0]\n",
    "\n",
    "row_grid, col_grid = np.meshgrid(np.arange(IMAGE_WIDTH), np.arange(IMAGE_HEIGHT), indexing='ij')\n",
    "mask = np.logical_or(row_grid < starting_point[0], np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "\n",
    "digits_list = [random_digits(fixed_label=d)[0] for d in range(10)]\n",
    "\n",
    "full_digits = np.hstack(digits_list)\n",
    "masked_digits = np.hstack([mask * d for d in digits_list])\n",
    "\n",
    "show_as_image(masked_digits + .75 * full_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAKhCAYAAABdKqmMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOSEAKPc/QZUpper3O7wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1179d57f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_starting_images = np.expand_dims(np.stack([mask * d for d in digits_list] * 10), axis=1)\n",
    "\n",
    "def batch_images_to_one(batches_images, n_square_elements=10):\n",
    "    rows_images = np.split(np.squeeze(batches_images), n_square_elements)\n",
    "    return np.vstack([np.hstack(row_images) for row_images in rows_images])\n",
    "\n",
    "samples = generate_samples(10, starting_point, batch_starting_images)\n",
    "\n",
    "show_as_image(\n",
    "    np.vstack([masked_digits, batch_images_to_one(samples)]),\n",
    "    figsize=(25, 12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
