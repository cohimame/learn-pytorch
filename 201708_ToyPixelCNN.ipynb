{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(binary_image, figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, utils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel CNN\n",
    "\n",
    "Alternative to Pixel RNN from [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf). \n",
    "\n",
    "On-line resources:\n",
    " * See for an existing PyTorch implementation https://github.com/jzbontar/pixelcnn-pytorch/blob/master/main.py\n",
    " * http://sergeiturukin.com/2017/02/22/pixelcnn.html for a nice walk-through\n",
    " * http://tinyclouds.org/residency/\n",
    " * https://tensorflow.blog/2016/11/29/pixelcnn-1601-06759-summary/ (in korean ;) ) \n",
    "\n",
    "The core ideas are the following:\n",
    "\n",
    "### Joint distribution of an image $\\mathbf{x}$ modelled as an autoregressive process\n",
    "\n",
    "Same model for PixelRNN and PixelCNN:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\prod_{i=1}^{n^2} p(x_i|x_{1}, \\dots, x_{i-1})$$\n",
    " \n",
    "![](http://sergeiturukin.com/assets/2017-02-22-183010_479x494_scrot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAElCAYAAACiZ/R3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABDFJREFUeJzt17Ftw0AUBUGeoRLk2CzC/VcgFqHcPZxTOxJpgNBCnolf\n8KPF3ZhzLgAVb88+AOAnUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlIuR8bX63Wu63rSKcAr\n27bta875/mh3KErrui632+3vVwH/1hjjvmfn+wakiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSI\nEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpA\niigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmi\nBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQ\nIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIoo\nASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSk\niBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpByefYBnGuM8ewT4BAvJSBFlIAUUQJSRAlIESUg\nRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRR\nAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlI\nESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgZcw594/H2D8G+G2bc34+\nGnkpASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEp\nogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgS\nkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCK\nKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIE\npIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAi\nSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigB\nKaIEpIgSkCJKQMrl4P5rWZb7GYcAL+9jz2jMOc8+BGA33zcgRZSAFFECUkQJSBElIEWUgBRRAlJE\nCUgRJSDlG/BoHd7iaQCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118098ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def causal_mask(width, height, include_center=False):\n",
    "    mask = np.ones(shape=(width, height))\n",
    "    mask[height // 2, width // 2 + include_center:] = 0\n",
    "    mask[height // 2 + 1:] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "print(causal_mask(5, 5))\n",
    "show_as_image(causal_mask(5, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  1.,   2.,   3.],\n",
       "         [  4.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[ 10.,  11.,  12.],\n",
       "         [ 13.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[ 19.,  20.,  21.],\n",
       "         [ 22.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[ 28.,  29.,  30.],\n",
       "         [ 31.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels, in_channels, width, height = 2, 2, 3, 3\n",
    "\n",
    "conv_weights = 1 + np.arange(out_channels * in_channels * width * height).reshape((out_channels, in_channels, width, height))\n",
    "\n",
    "masked_weights = conv_weights * causal_mask(width, height)\n",
    "\n",
    "masked_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully convolutional network preserving spatial resolution\n",
    "\n",
    "Input to output map      |  Output distribution\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://tensorflowkorea.files.wordpress.com/2016/11/pixel-cnn1.png)  |  ![](http://tinyclouds.org/residency/pixelcnn.png)\n",
    "\n",
    "Quite a counter-intuitive model:\n",
    "\n",
    " * Convolutional layers bottom to top!\n",
    " * Last layer with `kernel_size=1` and outputs $ n_W \\times n_H \\times n_{pixels}$ logits, inferring $p(\\mathbf{x})$ in one forward pass (during training)\n",
    " * Representation of dimension `n_channels` output by each layer anologous to RNN's internal state vector $\\mathbf{h}$\n",
    " * Necessary to stack enough layers (and/or dillatations) to augment the \"receptive field\" so that output pixels can be influenced by the whole image\n",
    " \n",
    "\n",
    "Below is a minimalistic implementation for 0/1 pixels without many of the bells and whistles of the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on a simple generative model of LCD digits\n",
    "\n",
    "From https://gist.github.com/benjaminwilson/b25a321f292f98d74269b83d4ed2b9a8#file-lcd-digits-dataset-nmf-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA2pJREFUeJzt3LGRAjEQAMFbijw+/ww+HXxy0IcAV3DAM922jDWmVDIk\nzVprg4rTuweAVxI8KYInRfCkCJ4UwZMieFIET4rgSdkV/Mz8HjUIPOLeNmfP1YKZcQ+Bj7XWmltr\nHGlIETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgifl/O4BjuAL\n8OeauflU9N+ww5MieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIE\nT4rgSfnKR9zf9OiY57LDkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4\nUgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQI\nnhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwp\ngidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRP\niuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwp553rr9u2XY4YBB70\nc8+iWWsdPQh8DEcaUgRPiuBJETwpgidF8KQInhTBkyJ4Uv4AmrcVqoHJrzkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b980400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CELL_LENGTH = 4\n",
    "IMAGE_WIDTH, IMAGE_HEIGHT = 2 * CELL_LENGTH + 5, CELL_LENGTH + 4\n",
    "\n",
    "def vertical_stroke(rightness, downness):\n",
    "    \"\"\"\n",
    "    Return a 2d numpy array representing an image with a single vertical stroke in it.\n",
    "    `rightness` and `downness` are values from [0, 1] and define the position of the vertical stroke.\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 2\n",
    "    j = rightness * (CELL_LENGTH + 1) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i + np.arange(CELL_LENGTH), j] = 1.\n",
    "    return x\n",
    "\n",
    "def horizontal_stroke(downness):\n",
    "    \"\"\"\n",
    "    Analogue to vertical_stroke, but it returns horizontal strokes.\n",
    "    `downness` is here a value in [0, 1, 2].\n",
    "    \"\"\"\n",
    "    i = (downness * (CELL_LENGTH + 1)) + 1\n",
    "    x = np.zeros(shape=(IMAGE_WIDTH, IMAGE_HEIGHT), dtype=np.float64)\n",
    "    x[i, 2 + np.arange(CELL_LENGTH)] = 1.\n",
    "    return x\n",
    "\n",
    "show_as_image(horizontal_stroke(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA6FJREFUeJzt3dFNwzAUQNE+1D3YfwPW4Z8dzAilIsEl95zvyLLkKysf\nee2stW5Q8bZ7A/CXBE+K4EkRPCmCJ0XwpAieFMGTInhSngp+Zj7O2gj8xk/bnGc+LZgZ3yHwstZa\n8+gZrzSkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZNy372B\nM/gJ8GPNPBwV/Tfc8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQI\nnhTBkyJ4UrYPcZ8xcH2loeNXcKUzcsOTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmC\nJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K\n4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGT\nInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0Xw\npAieFMGTInhS7rs3MDOHr7nWOnzNsjPOaBc3PCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAie\nFMGTInhSBE+K4EkRPCmCJ0XwpAielO1D3GcMXF9p6PgVXOmM3PCkCJ4UwZMieFIET4rgSRE8KYIn\nRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rg\nSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMi\neFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCk\nCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZNy372BmTl8zbXW4WuWnXFGu7jhSRE8KYInRfCkCJ4U\nwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkbB/iPsOVho45lhueFMGTInhS\nBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE/Ks0PcX7fb7fOMjcAv\nvf/kofE37ZR4pSFF8KQInhTBkyJ4UgRPiuBJETwpgiflG11eJ7gqM45BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ce0898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_STROKES = np.asarray(\n",
    "    [horizontal_stroke(k) for k in range(3)] + [vertical_stroke(k, l) for k in range(2) for l in range(2)])\n",
    "\n",
    "DIGITS_STROKES = np.array([[0, 2, 3, 4, 5, 6], [5, 6], [0, 1, 2, 4, 5], [0, 1, 2, 5, 6], [1, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 3, 5, 6], [0, 5, 6], np.arange(7), [0, 1, 2, 3, 5, 6]])\n",
    "\n",
    "def random_digits(strokes=BASE_STROKES, digit_as_strokes=DIGITS_STROKES, fixed_label=None):\n",
    "    label = fixed_label if fixed_label is not None else np.random.choice(len(digit_as_strokes))\n",
    "    combined_strokes = strokes[digit_as_strokes[label], :, :].sum(axis=0)\n",
    "    return combined_strokes, label\n",
    "\n",
    "\n",
    "x, label = random_digits()\n",
    "print(label)\n",
    "show_as_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " (0 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " \n",
       " (1 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " \n",
       " (2 ,0 ,.,.) = \n",
       "    0   0   0   0   0   0   0   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   1   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   0   0   0   0   1   0\n",
       "    0   0   1   1   1   1   0   0\n",
       "    0   0   0   0   0   0   0   0\n",
       " [torch.FloatTensor of size 3x1x13x8], \n",
       "  4\n",
       "  1\n",
       "  9\n",
       " [torch.LongTensor of size 3x1]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LcdDigits(Dataset):\n",
    "\n",
    "    def __init__(self, n_examples):\n",
    "        digits, labels = zip(*[random_digits() for _ in range(n_examples)])\n",
    "        self.digits = np.asarray(digits, dtype=np.float64)\n",
    "        self.labels = np.asarray(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        digit_with_channel = self.digits[idx][np.newaxis, :, :]\n",
    "        \n",
    "        return torch.from_numpy(digit_with_channel).float(), torch.from_numpy(np.array([self.labels[idx]]))\n",
    "\n",
    "next(b for b in DataLoader(LcdDigits(128), batch_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Iter [1/50] Loss: 0.6471\n",
      "Epoch [2/15], Iter [1/50] Loss: 0.1262\n",
      "Epoch [3/15], Iter [1/50] Loss: 0.0329\n",
      "Epoch [4/15], Iter [1/50] Loss: 0.0285\n",
      "Epoch [5/15], Iter [1/50] Loss: 0.0268\n",
      "Epoch [6/15], Iter [1/50] Loss: 0.0271\n",
      "Epoch [7/15], Iter [1/50] Loss: 0.0263\n",
      "Epoch [8/15], Iter [1/50] Loss: 0.0258\n",
      "Epoch [9/15], Iter [1/50] Loss: 0.0252\n",
      "Epoch [10/15], Iter [1/50] Loss: 0.0247\n",
      "Epoch [11/15], Iter [1/50] Loss: 0.0245\n",
      "Epoch [12/15], Iter [1/50] Loss: 0.0243\n",
      "Epoch [13/15], Iter [1/50] Loss: 0.0242\n",
      "Epoch [14/15], Iter [1/50] Loss: 0.0240\n",
      "Epoch [15/15], Iter [1/50] Loss: 0.0238\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "N_EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "train_dataset = LcdDigits(BATCH_SIZE * 50)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, N_EPOCHS, i+1, len(train_dataset)//BATCH_SIZE, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentially generating new samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA51JREFUeJzt3cFNw0AQQNEMSh/03wHtcKeHpQQSxWbB/72zZY3kr5UP\nnmTWWjeoeNs9APwmwZMieFIET4rgSRE8KYInRfCkCJ6Up4KfmY+zBoFXPNrmPPNpwcz4DoE/a601\nP13jlYYUwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFLuuwc4\ng58AP9bMj6ui/4YTnhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPyiWXuK+0dPwXnLEUv+sZOeFJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJ\nETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4\nUgRPiuBJETwp990DnGGttXuES5mZ3SMcxglPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF\n8KQInhTBkyJ4UgRPiuBJETwpgidl+xL3GQvXV1o65lhOeFIET4rgSRE8KYInRfCkCJ4UwZMieFIE\nT4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4U\nwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYIn\nRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rgSRE8KYInRfCkCJ4UwZMieFIET4rg\nSRE8KYInRfCkCJ4UwZMieFIET4rgSbnvHmBmDr/nWuvwe5ad8Yx2ccKTInhSBE+K4EkRPCmCJ0Xw\npAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EnZvsR9histHXMsJzwpgidF8KQInhTB\nkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInpRnl7i/brfb5xmDwIveH7lo\n/E07JV5pSBE8KYInRfCkCJ4UwZMieFIET4rgSfkG4eEhuLz4In4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1183254e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(n_samples, starting_point=(0, 0), starting_image=None):\n",
    "\n",
    "    samples = torch.from_numpy(\n",
    "        starting_image if starting_image is not None else np.zeros((n_samples * n_samples, 1, IMAGE_WIDTH, IMAGE_HEIGHT))).float()\n",
    "\n",
    "    cnn.train(False)\n",
    "\n",
    "    for i in range(IMAGE_WIDTH):\n",
    "        for j in range(IMAGE_HEIGHT):\n",
    "            if i < starting_point[0] or (i == starting_point[0] and j < starting_point[1]):\n",
    "                continue\n",
    "            out = cnn(Variable(samples, volatile=True))\n",
    "            probs = F.softmax(out[:, :, i, j]).data\n",
    "            samples[:, :, i, j] = torch.multinomial(probs, 1).float()\n",
    "    return samples.numpy()\n",
    "\n",
    "show_as_image(np.squeeze(generate_samples(n_samples=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or completing existing cropped image\n",
    "\n",
    " * $0, 8, 9$ and $2, 3, 7$ undistinguishable early one\n",
    " * Very small amount of noise (jitter) in samples\n",
    " * The last horizontal bar is hard to predict as it depends on the 1st horizontal bar\n",
    " * ($4, 9$) sometimes lead to incomplete or erroneous images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABwCAYAAAAdSHSxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAz9JREFUeJzt3cFR6zAUhlHpDS289WuCtpxmXBdFsKcHvS0r4hBbse9/\nzlpMfCOH+caMSB9jNACAVH9efQEAAK8khgCAaGIIAIgmhgCAaGIIAIgmhgCAaGIIAIgmhgCAaGII\nAIgmhgCAaG+PLO69++4OAOAqvsYYf+8t8mQIAKjqc8siMQQARBNDAEA0MQQARBNDAEC0h06T/dYY\ncw+h9d6nvl51M/dv9t65N6+t+v6Zbz+VZ2vNfM/yZAgAiCaGAIBoYggAiCaGAIBoYggAiCaGAIBo\nux2t/+mY3ZmO/FU42jx7vpnvWeXZWjtmvjPd79XnO8KZ5qv8+at+b5rvOZ4MAQDRxBAAEE0MAQDR\nxBAAEE0MAQDRxBAAEE0MAQDRxBAAEE0MAQDRxBAAEE0MAQDRxBAAEE0MAQDRdvvW+jO53W6vvoRD\nVZ6v8mytmY9zq7x/lWdrzXzP8mQIAIgmhgCAaGIIAIgmhgCAaGIIAIgmhgCAaLsdra9+rA/Oqvpn\nz3ycVfW9qz7fd54MAQDRxBAAEE0MAQDRxBAAEE0MAQDRxBAAEK2PMbYv7n37YspYlmXaa63rOu21\nWps7W2vm25v59jV7vsqq792F5vsYY7zfW+TJEAAQTQwBANHEEAAQTQwBANHEEAAQTQwBANF2O1r/\n0zG7Mx35q3B09Ij5qr9nZ1H9fTbftVWer/rvzTNdyxGemM/RegCAe8QQABBNDAEA0cQQABBNDAEA\n0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQ\nABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0cQQABBNDAEA0foYY/vi3rcv/mZZlt/8\n2K+t6zr19aqbuX+z9869eW3V96/6fJVV37sLzfcxxni/t8iTIQAgmhgCAKKJIQAgmhgCAKKJIQAg\nmhgCAKJNOVoPAPACjtYDANwjhgCAaGIIAIgmhgCAaGIIAIgmhgCAaG8Prv9qrX0ecSEAADv7t2XR\nQ/9nCACgGn8mAwCiiSEAIJoYAgCiiSEAIJoYAgCiiSEAIJoYAgCiiSEAIJoYAgCi/Qdh9NUBo3VK\nAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118743550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "starting_point = (4, 3)\n",
    "starting_image = random_digits(fixed_label=0)[0]\n",
    "\n",
    "row_grid, col_grid = np.meshgrid(np.arange(IMAGE_WIDTH), np.arange(IMAGE_HEIGHT), indexing='ij')\n",
    "mask = np.logical_or(row_grid < starting_point[0], np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "\n",
    "digits_list = [random_digits(fixed_label=d)[0] for d in range(10)]\n",
    "\n",
    "full_digits = np.hstack(digits_list)\n",
    "masked_digits = np.hstack([mask * d for d in digits_list])\n",
    "\n",
    "show_as_image(masked_digits + .75 * full_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAKhCAYAAABdKqmMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ef24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_starting_images = np.expand_dims(np.stack([mask * d for d in digits_list] * 10), axis=1)\n",
    "\n",
    "def batch_images_to_one(batches_images, n_square_elements=10):\n",
    "    rows_images = np.split(np.squeeze(batches_images), n_square_elements)\n",
    "    return np.vstack([np.hstack(row_images) for row_images in rows_images])\n",
    "\n",
    "samples = generate_samples(10, starting_point, batch_starting_images)\n",
    "\n",
    "show_as_image(\n",
    "    np.vstack([masked_digits, batch_images_to_one(samples)]),\n",
    "    figsize=(25, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
